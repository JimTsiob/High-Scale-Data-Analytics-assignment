{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65242,"databundleVersionId":7177791,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pandarallel\n!pip install datasketch\nprint(\"INSTALLATIONS COMPLETE.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T12:43:33.261557Z","iopub.execute_input":"2024-01-22T12:43:33.261978Z","iopub.status.idle":"2024-01-22T12:44:08.029209Z","shell.execute_reply.started":"2024-01-22T12:43:33.261945Z","shell.execute_reply":"2024-01-22T12:44:08.027361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,roc_auc_score,roc_curve\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom sklearn.ensemble import RandomForestClassifier\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom pandarallel import pandarallel\nfrom tqdm.notebook import tqdm\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import cross_validate, KFold, cross_val_score, StratifiedKFold , cross_val_predict\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport xgboost as xgb\nimport optuna\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import NearestNeighbors, VALID_METRICS_SPARSE\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn.metrics import jaccard_score\nfrom scipy import sparse\nfrom sklearn.preprocessing import normalize, binarize\n\nimport subprocess\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n    \n\nprint('Imports done.')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T12:44:08.033594Z","iopub.execute_input":"2024-01-22T12:44:08.034129Z","iopub.status.idle":"2024-01-22T12:44:12.586020Z","shell.execute_reply.started":"2024-01-22T12:44:08.034093Z","shell.execute_reply":"2024-01-22T12:44:12.584972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/bigdata2023classification/train.csv')\ntest_df = pd.read_csv('/kaggle/input/bigdata2023classification/test_without_labels.csv')\n\ntrain_df['Label_id'] = train_df['Label'].factorize()[0] # create specific ID for each label. Helps later on with wordclouds and XGBoost.\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T12:44:12.587229Z","iopub.execute_input":"2024-01-22T12:44:12.588511Z","iopub.status.idle":"2024-01-22T12:44:24.387680Z","shell.execute_reply.started":"2024-01-22T12:44:12.588471Z","shell.execute_reply":"2024-01-22T12:44:24.386479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 1.1","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Visualize our data (specifically the Label column)\n# Data Distribution\n\ntrain_df.groupby('Label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\nplt.gca().spines[['top', 'right',]].set_visible(False)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T12:44:24.390200Z","iopub.execute_input":"2024-01-22T12:44:24.390616Z","iopub.status.idle":"2024-01-22T12:44:24.705246Z","shell.execute_reply.started":"2024-01-22T12:44:24.390583Z","shell.execute_reply":"2024-01-22T12:44:24.703913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Countplot for column Label, there seem to be more entertainment articles.\nsns.countplot(x='Label',data=train_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T12:44:24.706645Z","iopub.execute_input":"2024-01-22T12:44:24.706988Z","iopub.status.idle":"2024-01-22T12:44:25.090442Z","shell.execute_reply.started":"2024-01-22T12:44:24.706960Z","shell.execute_reply":"2024-01-22T12:44:25.089222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check which ID corresponds to which label\nlabel = train_df[['Label', 'Label_id']].drop_duplicates().sort_values('Label_id')\nlabel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking shapes\nprint('train_df size: ', train_df.shape)\nprint('test_df size: ', test_df.shape)\n# 159707 full size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-01-18T09:17:02.852746Z","iopub.execute_input":"2024-01-18T09:17:02.853166Z","iopub.status.idle":"2024-01-18T09:22:33.827350Z","shell.execute_reply.started":"2024-01-18T09:17:02.853132Z","shell.execute_reply":"2024-01-18T09:22:33.825642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 1.2","metadata":{}},{"cell_type":"code","source":"# Text preprocessing for Content column\n# Lower all text\n\ntrain_df['Content'] = train_df['Content'].str.lower()\ntest_df['Content'] = test_df['Content'].str.lower()\n\n# Initialize pandarallel\n# I used pandarallel because it applies the functions much faster than a normal pandas apply.\npandarallel.initialize(nb_workers=8,progress_bar=True)\n\n# Remove all special characters\ndef remove_special_chars(text):\n    return ''.join(x if x.isalnum() else ' ' for x in text)\n\ntrain_df['Content'] = train_df['Content'].parallel_apply(remove_special_chars)\ntest_df['Content'] = test_df['Content'].parallel_apply(remove_special_chars)\n\n# get stopwords.\nstop = set(stopwords.words('english'))\nextra_stopwords = {'well', 'said', 'say', 'one', 'even'}\nstop.update(extra_stopwords)\n\n# Remove stop_words\ndef remove_stopwords(text):\n    words = word_tokenize(text)\n    return [x for x in words if x not in stop]\n\ntrain_df['Content'] = train_df['Content'].parallel_apply(remove_stopwords)\ntest_df['Content'] = test_df['Content'].parallel_apply(remove_stopwords)\n\n# Lemmatization\ndef lemmatize_word(text):\n    wordnet = WordNetLemmatizer()\n    return \" \".join([wordnet.lemmatize(word) for word in text])\n\ntrain_df['Content'] = train_df['Content'].parallel_apply(lemmatize_word)\ntest_df['Content'] = test_df['Content'].parallel_apply(lemmatize_word)\n\n\n\nprint('Example of preprocessing train: ')\nprint(train_df['Content'][0])\nprint(\"\\n\")\nprint('Example of preprocessing test: ')\nprint(test_df['Content'][0])","metadata":{"execution":{"iopub.status.busy":"2024-01-22T13:14:45.065658Z","iopub.execute_input":"2024-01-22T13:14:45.067071Z","iopub.status.idle":"2024-01-22T13:21:31.222827Z","shell.execute_reply.started":"2024-01-22T13:14:45.067009Z","shell.execute_reply":"2024-01-22T13:21:31.221502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word cloud creation\n\n# get each label's articles to create word cloud for each label\nentertainment = train_df[train_df['Label_id'] == 0]\nentertainment = entertainment['Content']\n\ntechnology = train_df[train_df['Label_id'] == 1]\ntechnology = technology['Content']\n\nbusiness = train_df[train_df['Label_id'] == 2]\nbusiness = business['Content']\n\nhealth = train_df[train_df['Label_id'] == 3]\nhealth = health['Content']\n\ndef wordcloud_draw(dataset, color = 'white'):\n    words = ' '.join(dataset)\n    cleaned_word = ' '.join([word for word in words.split()\n    if (word != 'news' and word != 'text')])\n    wordcloud = WordCloud(stopwords = stop,\n    background_color = color,\n    width = 2500, height = 2500).generate(cleaned_word)\n    plt.figure(1, figsize = (10,7))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()\n    \nprint('Entertainment wordcloud: ')\nwordcloud_draw(entertainment)\n\nprint('Technology wordcloud: ')\nwordcloud_draw(technology)\n\nprint('Business wordcloud: ')\nwordcloud_draw(business)\n\nprint('Health wordcloud: ')\nwordcloud_draw(health)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T13:21:31.226718Z","iopub.execute_input":"2024-01-22T13:21:31.227777Z","iopub.status.idle":"2024-01-22T13:27:41.560086Z","shell.execute_reply.started":"2024-01-22T13:21:31.227711Z","shell.execute_reply":"2024-01-22T13:27:41.558503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bag of words ","metadata":{}},{"cell_type":"code","source":"# Count vectorizer will represent the BOW model.\n\nX_train = train_df['Content']\nX_test = test_df['Content']\ny_train = train_df['Label']\n\ncount_vectorizer = CountVectorizer()\n\nX_train = count_vectorizer.fit_transform(train_df['Content'])\nX_test = count_vectorizer.transform(test_df['Content'])\n\nprint('Preprocessing complete.')","metadata":{"execution":{"iopub.status.busy":"2023-12-29T15:36:42.266594Z","iopub.status.idle":"2023-12-29T15:36:42.267442Z","shell.execute_reply.started":"2023-12-29T15:36:42.267155Z","shell.execute_reply":"2023-12-29T15:36:42.267181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM training and evaluation (with BOW)","metadata":{}},{"cell_type":"code","source":"# Support Vector Machine (SVM)\n# using LinearSVC because it's faster for large datasets , that can be separated linearly \n# and supports One-vs-Rest technique (Great for multi-class labels like our case). \n\nsvm_model = LinearSVC(random_state = 42, max_iter=1000)\n\n# Perform 5-fold cross-validation and get predictions for each fold\ny_pred = cross_val_predict(svm_model, X_train, y_train, cv=5)\n\n# Print classification report for each category\nprint(\"====================== SVM Classification Report (BOW) ======================\")\nprint(\"\\n\")\nprint(classification_report(y_train, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-23T11:31:02.073961Z","iopub.execute_input":"2023-12-23T11:31:02.075977Z","iopub.status.idle":"2023-12-23T11:35:28.057928Z","shell.execute_reply.started":"2023-12-23T11:31:02.075874Z","shell.execute_reply":"2023-12-23T11:35:28.056310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest training and evaluation (BOW)","metadata":{}},{"cell_type":"code","source":"# Creating a random forest classifier with 20 Decision trees. Took a small number of trees here because it's really slow with BOW.\nrf_model = RandomForestClassifier(n_estimators=20,n_jobs=-1)\n\n# Perform 5-fold cross-validation and get predictions for each fold\ny_pred = cross_val_predict(rf_model, X_train, y_train, cv=5)\n\n# Print classification report for each category\nprint(\"====================== Random Forest Classification Report (BOW) ======================\")\nprint(\"\\n\")\nprint(classification_report(y_train, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-23T11:38:00.803076Z","iopub.execute_input":"2023-12-23T11:38:00.804224Z","iopub.status.idle":"2023-12-23T11:45:52.263559Z","shell.execute_reply.started":"2023-12-23T11:38:00.804179Z","shell.execute_reply":"2023-12-23T11:45:52.261538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVD ","metadata":{}},{"cell_type":"code","source":"# Perform SVD\n\nsvd = TruncatedSVD(n_components=100)\nX_train_reduced = svd.fit_transform(X_train)\nX_test_reduced = svd.transform(X_test)\nprint(\"SVD complete.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-23T12:39:15.234952Z","iopub.execute_input":"2023-12-23T12:39:15.236295Z","iopub.status.idle":"2023-12-23T12:40:18.814298Z","shell.execute_reply.started":"2023-12-23T12:39:15.236233Z","shell.execute_reply":"2023-12-23T12:40:18.813250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM Training and evaluation (SVD)","metadata":{}},{"cell_type":"code","source":"# Support Vector Machine (SVM)\n# using LinearSVC because it's faster for large datasets , that can be separated linearly \n# and supports One-vs-Rest technique (Great for multi-class labels like our case). \n\nsvm_model = LinearSVC(random_state = 42, max_iter=1000)\n\n# Perform 5-fold cross-validation and get predictions for each fold\ny_pred = cross_val_predict(svm_model, X_train_reduced, y_train, cv=5)\n\n# Print classification report for each category\nprint(\"====================== SVM Classification Report (SVD) ======================\")\nprint(\"\\n\")\nprint(classification_report(y_train, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-23T12:40:18.816842Z","iopub.execute_input":"2023-12-23T12:40:18.817653Z","iopub.status.idle":"2023-12-23T13:01:37.618960Z","shell.execute_reply.started":"2023-12-23T12:40:18.817607Z","shell.execute_reply":"2023-12-23T13:01:37.617630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest training and evaluation (SVD)","metadata":{}},{"cell_type":"code","source":"# Creating a random forest classifier with 100 Decision trees (SVD cross validation is faster than BOW for Random Forest).\nrf_model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n\n# Perform 5-fold cross-validation and get predictions for each fold\ny_pred = cross_val_predict(rf_model, X_train_reduced, y_train, cv=5)\n\n# Print classification report for each category\nprint(\"====================== Random Forest Classification Report (SVD) ======================\")\nprint(\"\\n\")\nprint(classification_report(y_train, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-23T13:13:03.269185Z","iopub.execute_input":"2023-12-23T13:13:03.269656Z","iopub.status.idle":"2023-12-23T13:18:03.109457Z","shell.execute_reply.started":"2023-12-23T13:13:03.269623Z","shell.execute_reply":"2023-12-23T13:18:03.108173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Beat the benchmark algorithm","metadata":{}},{"cell_type":"code","source":"# Title added as feature with content (combined), increases accuracy a little bit (0.3% increase).\ntrain_df['Title'] = train_df['Title'].str.lower()\ntest_df['Title'] = test_df['Title'].str.lower()\n\npandarallel.initialize(nb_workers=8,progress_bar=True)\n\ntrain_df['Title'] = train_df['Title'].parallel_apply(remove_special_chars)\ntest_df['Title'] = test_df['Title'].parallel_apply(remove_special_chars)\n\ntrain_df['Title'] = train_df['Title'].parallel_apply(remove_stopwords)\ntest_df['Title'] = test_df['Title'].parallel_apply(remove_stopwords)\n\ntrain_df['Title'] = train_df['Title'].parallel_apply(lemmatize_word)\ntest_df['Title'] = test_df['Title'].parallel_apply(lemmatize_word)\n\ntrain_df['Combined'] = train_df['Title'] + ' ' + train_df['Content']\ntest_df['Combined'] = test_df['Title'] + ' ' + test_df['Content']\n\nprint('Example of preprocessing train: ')\nprint(train_df['Combined'][0])\nprint(\"\\n\")\nprint('Example of preprocessing test: ')\nprint(test_df['Combined'][0])","metadata":{"execution":{"iopub.status.busy":"2024-01-18T09:30:50.276920Z","iopub.execute_input":"2024-01-18T09:30:50.277311Z","iopub.status.idle":"2024-01-18T09:31:34.027308Z","shell.execute_reply.started":"2024-01-18T09:30:50.277277Z","shell.execute_reply":"2024-01-18T09:31:34.025674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_df['Combined']\nX_test = test_df['Combined']\ny_train = train_df['Label']\n\n# Using hashing vectorizer as it's a better form of the classic BOW count vectorizer,\n# giving me the best possible results.\n\nhashing_vectorizer = HashingVectorizer()\nX_train = hashing_vectorizer.fit_transform(train_df['Combined'])\nX_test = hashing_vectorizer.transform(test_df['Combined'])\n\nprint('Preprocessing complete.')","metadata":{"execution":{"iopub.status.busy":"2024-01-18T09:31:34.032044Z","iopub.execute_input":"2024-01-18T09:31:34.033220Z","iopub.status.idle":"2024-01-18T09:32:19.485974Z","shell.execute_reply.started":"2024-01-18T09:31:34.033175Z","shell.execute_reply":"2024-01-18T09:32:19.484375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression model (BOW) , testing this model but it gave me worse results than LinearSVC with hashing vectorizer.\n# Using sag solver since it finishes the fastest out of all solvers (along with saga, 2 minutes)\n\nlr_model = LogisticRegression(tol=1e-4, C=1.0, n_jobs=-1, solver='sag',random_state=42)\n\n# Perform 5-fold cross-validation and get predictions for each fold\ny_pred = cross_val_predict(lr_model, X_train, y_train, cv=5)\n\n# Print classification report for each category\nprint(\"====================== Logistic Regression Classification Report ======================\")\nprint(\"\\n\")\nprint(classification_report(y_train, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-23T13:24:01.943706Z","iopub.execute_input":"2023-12-23T13:24:01.944209Z","iopub.status.idle":"2023-12-23T13:26:32.544590Z","shell.execute_reply.started":"2023-12-23T13:24:01.944171Z","shell.execute_reply":"2023-12-23T13:26:32.543403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost model\nxgb_model = xgb.XGBClassifier(learning_rate=0.1, n_estimators=20, max_depth=3, random_state=42)\n\ny_label_ids = train_df['Label_id']\n\n# Perform 5-fold cross-validation and get predictions for each fold\ny_pred = cross_val_predict(xgb_model, X_train, y_label_ids, cv=5)\n\n# Print classification report for each category\nprint(\"====================== XGBoost Classification Report ======================\")\nprint(\"\\n\")\nprint(classification_report(y_label_ids, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-23T14:05:17.298612Z","iopub.execute_input":"2023-12-23T14:05:17.299449Z","iopub.status.idle":"2023-12-23T14:13:29.286636Z","shell.execute_reply.started":"2023-12-23T14:05:17.299403Z","shell.execute_reply":"2023-12-23T14:13:29.285276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Machine (SVM)\n# using LinearSVC because it's faster for large datasets , that can be separated linearly \n# and supports One-vs-Rest technique (Great for multi-class labels like our case). \n\n# Best model with 97% accuracy\n\nsvm_model = LinearSVC(random_state=42, max_iter=1000)\n\n# Perform 5-fold cross-validation and get predictions for each fold\ny_pred = cross_val_predict(svm_model, X_train, y_train, cv=5)\n\n# Print classification report for each category\nprint(\"====================== SVM Classification Report (Best model) ======================\")\nprint(\"\\n\")\nprint(classification_report(y_train, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:48:31.713658Z","iopub.execute_input":"2023-12-23T16:48:31.714079Z","iopub.status.idle":"2023-12-23T16:49:16.260366Z","shell.execute_reply.started":"2023-12-23T16:48:31.714046Z","shell.execute_reply":"2023-12-23T16:49:16.259170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Obtain predictions","metadata":{}},{"cell_type":"code","source":"svm_model.fit(X_train,y_train)\n\ny_pred_test = svm_model.predict(X_test)\n\nprint('Training complete.')","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:49:16.262213Z","iopub.execute_input":"2023-12-23T16:49:16.262642Z","iopub.status.idle":"2023-12-23T16:49:25.278627Z","shell.execute_reply.started":"2023-12-23T16:49:16.262607Z","shell.execute_reply":"2023-12-23T16:49:25.277542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Output file","metadata":{}},{"cell_type":"code","source":"import csv\n\nwith open('testSet_categories.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    \n    writer.writerow([\"Id\",\"Predicted\"])\n    for i in range(0,47912):\n        writer.writerow([test_df['Id'][i],y_pred_test[i]])  \n    \n    \nprint(\"CSV file writing complete.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:49:25.280149Z","iopub.execute_input":"2023-12-23T16:49:25.280617Z","iopub.status.idle":"2023-12-23T16:49:25.979740Z","shell.execute_reply.started":"2023-12-23T16:49:25.280574Z","shell.execute_reply":"2023-12-23T16:49:25.978435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question 2","metadata":{}},{"cell_type":"markdown","source":"## Brute force k-NN","metadata":{}},{"cell_type":"code","source":"import time\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\n\n# turn hash-vectorized sparse matrices to SVD arrays in order to get faster computation.\nsvd = TruncatedSVD(n_components=100)\nX_train_reduced = svd.fit_transform(X_train)\nX_test_reduced = svd.transform(X_test)\n\n# Make SVD arrays to binary format for proper jaccard computation.\nX_train_binary = binarize(X_train_reduced)\nX_test_binary = binarize(X_test_reduced)\n\nprint(\"x_train_binary shape: \", X_train_binary.shape)\n\nprint('svd done.')\n\nstart_time = time.time()\nnbrs = NearestNeighbors(n_neighbors=15, algorithm='brute', metric='jaccard', n_jobs=-1).fit(X_train_binary)\nbuild_time = time.time() - start_time\nprint(f\"Build time: {build_time} seconds\")\n\nprint('nbrs done.')\n\n# Suppress DataConversionWarning for this block , to save some room\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=DataConversionWarning)\n\n    start_time = time.time()\n    distances, indices = nbrs.kneighbors(X_test_binary)\n    query_time = time.time() - start_time\n    print(f\"Query time: {query_time} seconds\")\n\n\nprint('k-nn done.')","metadata":{"execution":{"iopub.status.busy":"2024-01-18T09:32:19.489211Z","iopub.execute_input":"2024-01-18T09:32:19.489764Z","iopub.status.idle":"2024-01-18T09:50:22.288917Z","shell.execute_reply.started":"2024-01-18T09:32:19.489715Z","shell.execute_reply":"2024-01-18T09:50:22.286562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking NearestNeighbors validity (if what is found is correct)\n# Both articles seem to be correct, since they're business related.\nprint(\"Source Example:\")\nprint(test_df.iloc[1]['Combined'])\n\nprint('\\n')\nprint(\"Neighbor:\")\nprint(train_df.iloc[indices[1][0]]['Combined'])\n\nprint('\\n')\nprint(indices[1])","metadata":{"execution":{"iopub.status.busy":"2024-01-18T09:50:40.737552Z","iopub.execute_input":"2024-01-18T09:50:40.739016Z","iopub.status.idle":"2024-01-18T09:50:40.749702Z","shell.execute_reply.started":"2024-01-18T09:50:40.738964Z","shell.execute_reply":"2024-01-18T09:50:40.747651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Min-hash LSH","metadata":{}},{"cell_type":"code","source":"# Tokenize and create sets of tokens\n\npandarallel.initialize(nb_workers=8,progress_bar=True)\n\ndef tokenize_text(text):\n    return set(word_tokenize(text))\n\n# Tokenize and create sets of tokens for test_df and train_df\ntrain_df['Tokenized'] = train_df['Combined'].parallel_apply(lambda x: tokenize_text(x))\ntest_df['Tokenized'] = test_df['Combined'].parallel_apply(lambda x: tokenize_text(x))\n\n# Now train_df['Tokenized'] contains sets of tokens for each article\nprint(train_df['Tokenized'][0])","metadata":{"execution":{"iopub.status.busy":"2024-01-18T09:50:48.183351Z","iopub.execute_input":"2024-01-18T09:50:48.183915Z","iopub.status.idle":"2024-01-18T09:54:27.665821Z","shell.execute_reply.started":"2024-01-18T09:50:48.183871Z","shell.execute_reply":"2024-01-18T09:54:27.664019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Min Hash LSH\n\nfrom datasketch import MinHash, MinHashLSH\n\nminhash_objects = []\n\ndef create_minhash(tokens):\n    minhash = MinHash(num_perm=16)\n    for token in tokens:\n        minhash.update(token.encode('utf8'))\n    return minhash\n    \n\nstart_time = time.time()\n\n\nminhash_objects = list(map(create_minhash, train_df['Tokenized']))\n\n# Create MinHashLSH Index\n# lsh = MinHashLSH(threshold=0.8, num_perm=16)\n\n# for i, minhash in enumerate(minhash_objects):\n#     lsh.insert(i, minhash)\n\n\ntest_minhash_objects = []\n# Create MinHash objects for test_df\ntest_minhash_objects = list(map(create_minhash, test_df['Tokenized']))\n\n\n# Query MinHashLSH for candidates in train_df\n\n\n# Create MinHashLSH Index\nlsh = MinHashLSH(threshold=0.8, num_perm=16)\n\nfor i, minhash in enumerate(minhash_objects):\n    lsh.insert(i, minhash)\n    \nbuild_time = time.time() - start_time\nprint(f\"Build time (lsh with 16 permutations, threshold >= 0.8): {build_time} seconds\")\n# test_candidates = []\n# for i, minhash in enumerate(test_minhash_objects):\n#     result = lsh.query(minhash)\n#     test_candidates.append(result)\nstart_time = time.time()\ntest_candidates = [lsh.query(minhash) for minhash in test_minhash_objects]\n\nquery_time = time.time() - start_time\nprint(f\"Query time (lsh with 16 permutations, threshold >= 0.8): {query_time} seconds\")\n\n# num_neighbors = 15\n\n# neighbors = []\n# for i, minhash in enumerate(minhash_objects):\n#     result = lsh.query(minhash)\n# #     print(f\"Neighbors for document {i + 1} ({df['document_id'][i]}): {result}\")\n#     neighbors.append(result)\n\nprint('min hash done.')","metadata":{"execution":{"iopub.status.busy":"2024-01-18T09:54:27.668848Z","iopub.execute_input":"2024-01-18T09:54:27.669980Z","iopub.status.idle":"2024-01-18T10:04:27.775337Z","shell.execute_reply.started":"2024-01-18T09:54:27.669931Z","shell.execute_reply":"2024-01-18T10:04:27.773631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_candidates_flat = test_candidates.flatten(), jic -> / 1000\ntest_candidates_flat = [item for sublist in test_candidates for item in sublist]\nprint(len(test_candidates_flat))\nindices_flat = indices.flatten()\nprint(\"Fraction of true k-most similar docs found by LSH (16 perms): \", len(set(test_candidates_flat).intersection(indices_flat)) / 15) # find fraction for K=15 ","metadata":{"execution":{"iopub.status.busy":"2024-01-18T10:10:36.780165Z","iopub.execute_input":"2024-01-18T10:10:36.780816Z","iopub.status.idle":"2024-01-18T10:10:36.946952Z","shell.execute_reply.started":"2024-01-18T10:10:36.780768Z","shell.execute_reply":"2024-01-18T10:10:36.945610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 32 permutations minhash LSH\nminhash_objects_32 = []\n\ndef create_minhash(tokens):\n    minhash = MinHash(num_perm=32)\n    for token in tokens:\n        minhash.update(token.encode('utf8'))\n    return minhash\n\nstart_time = time.time()\nminhash_objects_32 = list(map(create_minhash, train_df['Tokenized']))\n\n# Create MinHashLSH Index\nlsh2 = MinHashLSH(threshold=0.8, num_perm=32)\n\nfor i, minhash in enumerate(minhash_objects_32):\n    lsh2.insert(i, minhash)\n\nbuild_time = time.time() - start_time\nprint(f\"Build time (lsh with 32 permutations, threshold >= 0.8): {build_time} seconds\")\n\n\nstart_time = time.time()\n# Create MinHash objects for test_df\ntest_minhash_objects_32 = list(map(create_minhash, test_df['Tokenized']))\n\n# Query MinHashLSH for candidates in train_df\ntest_candidates_32 = [lsh2.query(minhash) for minhash in test_minhash_objects_32]\n\ndups_dict_32 = {}\n\nfor i,minhash in enumerate(test_candidates_32):\n    dups_dict_32[i] = minhash\n\nquery_time = time.time() - start_time\nprint(f\"Query time (lsh with 32 permutations, threshold >= 0.8): {query_time} seconds\")\n\nmax_length = len(max(test_candidates_32, key=len))\nprint(\"length of list of neighbors with most neighbors (32 permutations minhash LSH):\", max_length)\n\nsum_of_lengths = sum(len(sublist) for sublist in test_candidates_32)\nprint(\"Sum of neighbors (32 permutations): \", sum_of_lengths)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T10:10:41.494433Z","iopub.execute_input":"2024-01-18T10:10:41.494905Z","iopub.status.idle":"2024-01-18T10:21:27.563423Z","shell.execute_reply.started":"2024-01-18T10:10:41.494858Z","shell.execute_reply":"2024-01-18T10:21:27.561833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_candidates_flat = test_candidates.flatten(), , jic -> / 1000\ntest_candidates_flat_32 = [item for sublist in test_candidates_32 for item in sublist]\nprint(len(test_candidates_flat_32))\nindices_flat = indices.flatten()\nprint(\"Fraction of true k-most similar docs found by LSH (32 perms): \", len(set(test_candidates_flat_32).intersection(indices_flat)) / 15) # find fraction for K=15","metadata":{"execution":{"iopub.status.busy":"2024-01-18T10:21:27.565894Z","iopub.execute_input":"2024-01-18T10:21:27.566331Z","iopub.status.idle":"2024-01-18T10:21:27.740245Z","shell.execute_reply.started":"2024-01-18T10:21:27.566297Z","shell.execute_reply":"2024-01-18T10:21:27.738632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 64 permutations minhash LSH\nminhash_objects_64 = []\n\ndef create_minhash(tokens):\n    minhash = MinHash(num_perm=64)\n    for token in tokens:\n        minhash.update(token.encode('utf8'))\n    return minhash\n\nstart_time = time.time()\n\nminhash_objects_64 = list(map(create_minhash, train_df['Tokenized']))\n\n# Create MinHashLSH Index\nlsh3 = MinHashLSH(threshold=0.8, num_perm=64)\n\nfor i, minhash in enumerate(minhash_objects_64):\n    lsh3.insert(i, minhash)\n\nbuild_time = time.time() - start_time\nprint(f\"Build time (lsh with 64 permutations, threshold >= 0.8): {build_time} seconds\")\n\nstart_time = time.time()\n# Create MinHash objects for test_df\ntest_minhash_objects_64 = list(map(create_minhash, test_df['Tokenized']))\n\n# Query MinHashLSH for candidates in train_df\ntest_candidates_64 = [lsh3.query(minhash) for minhash in test_minhash_objects_64]\n\nquery_time = time.time() - start_time\nprint(f\"Query time (lsh with 64 permutations, threshold >= 0.8): {query_time} seconds\")\n\n\nmax_length = len(max(test_candidates_64, key=len))\nprint(\"length of list of neighbors with most neighbors (64 permutations minhash LSH):\", max_length)\n# print(test_candidates)\nsum_of_lengths = sum(len(sublist) for sublist in test_candidates_64)\nprint(\"Sum of neighbors (64 permutations): \", sum_of_lengths)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T10:42:06.265046Z","iopub.execute_input":"2024-01-11T10:42:06.265537Z","iopub.status.idle":"2024-01-11T10:47:59.573707Z","shell.execute_reply.started":"2024-01-11T10:42:06.265509Z","shell.execute_reply":"2024-01-11T10:47:59.572727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# jic -> / 1000\ntest_candidates_flat_64 = [item for sublist in test_candidates_64 for item in sublist]\nprint(len(test_candidates_flat_64))\nindices_flat = indices.flatten()\nprint(\"Fraction of true k-most similar docs found by LSH (64 perms): \", len(set(test_candidates_flat_64).intersection(indices_flat)) / 15) # find fraction for K=15","metadata":{"execution":{"iopub.status.busy":"2024-01-11T10:50:20.409525Z","iopub.execute_input":"2024-01-11T10:50:20.410265Z","iopub.status.idle":"2024-01-11T10:50:20.474355Z","shell.execute_reply.started":"2024-01-11T10:50:20.410212Z","shell.execute_reply":"2024-01-11T10:50:20.473024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 16 permutations minhash LSH with >= 0.5 threshold\nminhash_objects = []\n\ndef create_minhash(tokens):\n    minhash = MinHash(num_perm=16)\n    for token in tokens:\n        minhash.update(token.encode('utf8'))\n    return minhash\n\nstart_time = time.time()\nminhash_objects = list(map(create_minhash, train_df['Tokenized']))\n\n# Create MinHashLSH Index\nlsh = MinHashLSH(threshold=0.5, num_perm=16)\n\nfor i, minhash in enumerate(minhash_objects):\n    lsh.insert(i, minhash)\n\nbuild_time = time.time() - start_time\nprint(f\"Build time (lsh with 16 permutations, threshold >= 0.5): {build_time} seconds\")\n\n# Create MinHash objects for test_df\nstart_time = time.time()\ntest_minhash_objects = list(map(create_minhash, test_df['Tokenized']))\n\n# Query MinHashLSH for candidates in train_df\n    \ntest_candidates = [lsh.query(minhash) for minhash in test_minhash_objects]    \n\nquery_time = time.time() - start_time\nprint(f\"Query time (lsh with 16 permutations, threshold >= 0.5): {query_time} seconds\")\n\n# print(test_candidates)\nprint('min hash done.')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:09:06.584144Z","iopub.execute_input":"2024-01-11T11:09:06.585220Z","iopub.status.idle":"2024-01-11T11:14:13.728894Z","shell.execute_reply.started":"2024-01-11T11:09:06.585180Z","shell.execute_reply":"2024-01-11T11:14:13.727651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# , jic -> / 100\ntest_candidates_flat = [item for sublist in test_candidates for item in sublist]\nprint(len(test_candidates_flat))\nindices_flat = indices.flatten()\nprint(\"Fraction of true k-most similar docs found by LSH (16 perms, >= 0.5 threshold): \", len(set(test_candidates_flat).intersection(indices_flat)) / 15) # find fraction for K=15","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:16:30.345530Z","iopub.execute_input":"2024-01-11T11:16:30.346649Z","iopub.status.idle":"2024-01-11T11:16:30.641014Z","shell.execute_reply.started":"2024-01-11T11:16:30.346607Z","shell.execute_reply":"2024-01-11T11:16:30.640107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 32 permutations minhash LSH with >= 0.5 threshold\nminhash_objects_32 = []\n\ndef create_minhash(tokens):\n    minhash = MinHash(num_perm=32)\n    for token in tokens:\n        minhash.update(token.encode('utf8'))\n    return minhash\n\nstart_time = time.time()\nminhash_objects_32 = list(map(create_minhash, train_df['Tokenized']))\n\n# Create MinHashLSH Index\nlsh2 = MinHashLSH(threshold=0.5, num_perm=32)\n\nfor i, minhash in enumerate(minhash_objects_32):\n    lsh2.insert(i, minhash)\n\nbuild_time = time.time() - start_time\nprint(f\"Build time (lsh with 32 permutations, threshold >= 0.5): {build_time} seconds\")\n\n# Create MinHash objects for test_df\nstart_time = time.time()\ntest_minhash_objects_32 = list(map(create_minhash, test_df['Tokenized']))\n\n# Query MinHashLSH for candidates in train_df\n    \ntest_candidates_32 = [lsh2.query(minhash) for minhash in test_minhash_objects_32]    \n\nquery_time = time.time() - start_time\nprint(f\"Query time (lsh with 32 permutations, threshold >= 0.5): {query_time} seconds\")\n\n# print(test_candidates)\nprint('min hash done.')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:17:40.082097Z","iopub.execute_input":"2024-01-11T11:17:40.082732Z","iopub.status.idle":"2024-01-11T11:23:04.519855Z","shell.execute_reply.started":"2024-01-11T11:17:40.082700Z","shell.execute_reply":"2024-01-11T11:23:04.518689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# , jic -> / 100\ntest_candidates_flat_32 = [item for sublist in test_candidates_32 for item in sublist]\nprint(len(test_candidates_flat_32))\nindices_flat = indices.flatten()\nprint(\"Fraction of true k-most similar docs found by LSH (32 perms, >= 0.5 threshold): \", len(set(test_candidates_flat_32).intersection(indices_flat)) / 15) # find fraction for K=15","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:23:08.867871Z","iopub.execute_input":"2024-01-11T11:23:08.868893Z","iopub.status.idle":"2024-01-11T11:23:08.987477Z","shell.execute_reply.started":"2024-01-11T11:23:08.868862Z","shell.execute_reply":"2024-01-11T11:23:08.986294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 64 permutations minhash LSH with >= 0.5 threshold\nminhash_objects_64 = []\n\ndef create_minhash(tokens):\n    minhash = MinHash(num_perm=64)\n    for token in tokens:\n        minhash.update(token.encode('utf8'))\n    return minhash\n\nstart_time = time.time()\nminhash_objects_64 = list(map(create_minhash, train_df['Tokenized']))\n\n# Create MinHashLSH Index\nlsh3 = MinHashLSH(threshold=0.5, num_perm=64)\n\nfor i, minhash in enumerate(minhash_objects_64):\n    lsh3.insert(i, minhash)\n\nbuild_time = time.time() - start_time\nprint(f\"Build time (lsh with 64 permutations, threshold >= 0.5): {build_time} seconds\")\n\n# Create MinHash objects for test_df\nstart_time = time.time()\ntest_minhash_objects_64 = list(map(create_minhash, test_df['Tokenized']))\n\n# Query MinHashLSH for candidates in train_df\n    \ntest_candidates_64 = [lsh3.query(minhash) for minhash in test_minhash_objects_64]    \n\nquery_time = time.time() - start_time\nprint(f\"Query time (lsh with 64 permutations, threshold >= 0.5): {query_time} seconds\")\n\n# print(test_candidates)\nprint('min hash done.')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:23:39.883217Z","iopub.execute_input":"2024-01-11T11:23:39.883549Z","iopub.status.idle":"2024-01-11T11:29:39.308054Z","shell.execute_reply.started":"2024-01-11T11:23:39.883527Z","shell.execute_reply":"2024-01-11T11:29:39.306617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# jic -> / 100\ntest_candidates_flat_64 = [item for sublist in test_candidates_64 for item in sublist]\nprint(len(test_candidates_flat_64))\nindices_flat = indices.flatten()\nprint(\"Fraction of true k-most similar docs found by LSH (64 perms, >= 0.5 threshold): \", len(set(test_candidates_flat_64).intersection(indices_flat)) / 15) # find fraction for K=15","metadata":{"execution":{"iopub.status.busy":"2024-01-11T11:29:42.840409Z","iopub.execute_input":"2024-01-11T11:29:42.841454Z","iopub.status.idle":"2024-01-11T11:29:43.014689Z","shell.execute_reply.started":"2024-01-11T11:29:42.841416Z","shell.execute_reply":"2024-01-11T11:29:43.013704Z"},"trusted":true},"execution_count":null,"outputs":[]}]}